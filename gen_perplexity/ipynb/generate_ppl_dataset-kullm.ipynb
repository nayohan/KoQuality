{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uj-user/Yo/HiT5/HCLT/hclt-venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-08 02:55:53,200] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'instruction', 'input', 'output', 'input_ids', 'token_type_ids', 'attention_mask', 'trunc_instruction'],\n",
      "        num_rows: 145175\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import evaluate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# dataset_name = 'beomi/KoAlpaca-v1.1a'\n",
    "dataset_name = 'nlpai-lab/kullm-v2'\n",
    "# dataset_name = 'nlpai-lab/kullm-v2'\n",
    "# dataset_name = 'junelee/sharegpt_deepl_ko'\n",
    "\n",
    "model_name = 'EleutherAI/polyglot-ko-1.3b'\n",
    "# model_name = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'bos_token':'<|endoftext|>'})\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.filter(lambda example: len(example['instruction'])>0, num_proc=24) # filtering under 0 token length\n",
    "\n",
    "# filtering over 1024 token length\n",
    "def encode_preprocess(examples):\n",
    "    return tokenizer(examples['instruction'])#, padding=True, return_tensors='pt')\n",
    "\n",
    "# truncate max length, add padding true\n",
    "def encode_pad_preprocess(examples):\n",
    "    return tokenizer(examples['instruction'], max_length=512, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# extract truncated sentence max_length 1024\n",
    "def decode_process(examples):\n",
    "    return {'trunc_instruction': tokenizer.decode(examples['input_ids'], skip_special_tokens=True)}\n",
    "\n",
    "trunc_data = dataset.map(encode_preprocess, batched=True, num_proc=24)\n",
    "trunc_data = trunc_data.filter(lambda example: len(example['input_ids'])<512, num_proc=24)\n",
    "encode_pad_data = trunc_data.map(encode_pad_preprocess, batched=True, num_proc=24)\n",
    "preprocessed_data = encode_pad_data.map(decode_process, num_proc=96)\n",
    "print(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    145175.000000\n",
       "mean         63.464481\n",
       "std         111.533839\n",
       "min           1.000000\n",
       "25%          23.000000\n",
       "50%          32.000000\n",
       "75%          52.000000\n",
       "max        1245.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(preprocessed_data['train']['instruction'])[0].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.92s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m instruction\n\u001b[1;32m      5\u001b[0m len_instruction \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m instruction]\n\u001b[0;32m----> 6\u001b[0m ppl_results \u001b[39m=\u001b[39m perplexity\u001b[39m.\u001b[39;49mcompute(model_id\u001b[39m=\u001b[39;49mmodel_name, add_start_token\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, predictions\u001b[39m=\u001b[39;49minstruction)\n\u001b[1;32m      7\u001b[0m round_ppl_results \u001b[39m=\u001b[39m [\u001b[39mround\u001b[39m(ppl, \u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m ppl \u001b[39min\u001b[39;00m  ppl_results[\u001b[39m'\u001b[39m\u001b[39mperplexities\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/Yo/HiT5/HCLT/hclt-venv/lib/python3.8/site-packages/evaluate/module.py:444\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    443\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[0;32m--> 444\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs)\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--perplexity/8ab643ad86f568b7d1d5f7822373fa7401ff5ff0297ccf114b0ca6a33be96bc0/perplexity.py:157\u001b[0m, in \u001b[0;36mPerplexity._compute\u001b[0;34m(self, predictions, model_id, batch_size, add_start_token, device, max_length)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mall(torch\u001b[39m.\u001b[39mge(attn_masks\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m), \u001b[39m1\u001b[39m)), \u001b[39m\"\u001b[39m\u001b[39mEach input text must be at least one token long.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mall(\n\u001b[1;32m    158\u001b[0m         torch\u001b[39m.\u001b[39mge(attn_masks\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m    159\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mWhen add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m ppls \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss(reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "instruction = preprocessed_data['train']['trunc_instruction']\n",
    "instruction\n",
    "\n",
    "len_instruction = [len(text) for text in instruction]\n",
    "ppl_results = perplexity.compute(model_id=model_name, add_start_token=False, predictions=instruction)\n",
    "round_ppl_results = [round(ppl, 2) for ppl in  ppl_results['perplexities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>ppl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"(주)회사명과 회사명(주)의 차이점은 무엇인가요?\"\\n\\n 질문 본문: \"주식회사...</td>\n",
       "      <td>16.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"-에요\"와 \"-예요\"의 쓰임에 대해서 자세히 설명해주세요.</td>\n",
       "      <td>23.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"..등이 있다.\", \"..등등이 있다.\"할때 등과 등등은 무슨 차이가 있나요? 의...</td>\n",
       "      <td>17.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"02:42:35 OST\" 이런 식으로 시간 뒤에 붙는 OST는 무슨 뜻인가요?</td>\n",
       "      <td>99.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"1+1=0\" 이라는 문구를 어디서 본 것 같은데, 어디서 봤는지 전혀 기억이 나지...</td>\n",
       "      <td>10.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21145</th>\n",
       "      <td>힙합 음악에서 자주 사용되는 용어인 플로우, 펀치라인, 그리고 라인의 의미가 무엇인...</td>\n",
       "      <td>16.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21146</th>\n",
       "      <td>힙합 패션을 입을 때 세미힙합, 무난한 캐쥬얼, 리얼힙합을 고민하고 있습니다. 어떤...</td>\n",
       "      <td>29.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21147</th>\n",
       "      <td>힙합(랩)에 대해 기본적인 지식과 용어, 그리고 영향력 있는 곡들을 알고 싶습니다.</td>\n",
       "      <td>14.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21148</th>\n",
       "      <td>힙합에서 자주 쓰이는 용어 'AKA'는 무슨 뜻인가요?</td>\n",
       "      <td>17.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21149</th>\n",
       "      <td>힙합에서 트랩이란 무엇인가요? 그리고 대충 어떤 느낌인가요?</td>\n",
       "      <td>34.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction    ppl\n",
       "0      \"(주)회사명과 회사명(주)의 차이점은 무엇인가요?\"\\n\\n 질문 본문: \"주식회사...  16.44\n",
       "1                      \"-에요\"와 \"-예요\"의 쓰임에 대해서 자세히 설명해주세요.  23.21\n",
       "2      \"..등이 있다.\", \"..등등이 있다.\"할때 등과 등등은 무슨 차이가 있나요? 의...  17.52\n",
       "3           \"02:42:35 OST\" 이런 식으로 시간 뒤에 붙는 OST는 무슨 뜻인가요?  99.94\n",
       "4      \"1+1=0\" 이라는 문구를 어디서 본 것 같은데, 어디서 봤는지 전혀 기억이 나지...  10.19\n",
       "...                                                  ...    ...\n",
       "21145  힙합 음악에서 자주 사용되는 용어인 플로우, 펀치라인, 그리고 라인의 의미가 무엇인...  16.16\n",
       "21146  힙합 패션을 입을 때 세미힙합, 무난한 캐쥬얼, 리얼힙합을 고민하고 있습니다. 어떤...  29.49\n",
       "21147     힙합(랩)에 대해 기본적인 지식과 용어, 그리고 영향력 있는 곡들을 알고 싶습니다.  14.79\n",
       "21148                     힙합에서 자주 쓰이는 용어 'AKA'는 무슨 뜻인가요?  17.68\n",
       "21149                  힙합에서 트랩이란 무엇인가요? 그리고 대충 어떤 느낌인가요?  34.47\n",
       "\n",
       "[21150 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ppl_instruction = dict(zip(instruction, round_ppl_results))\n",
    "df_dataset = pd.DataFrame(sorted(dict_ppl_instruction.items(), key=lambda x: x[0]), columns=['instruction', 'ppl'])\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname = dataset_name.split('/')[-1]\n",
    "df_dataset.to_json(f\"{datasetname}_ppl.json\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'url', 'token_type_ids', 'ppl', 'len'],\n",
       "    num_rows: 21155\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = preprocessed_data['train'].add_column(\"ppl\", round_ppl_results)\n",
    "dataset = dataset.add_column(\"len\", len_instruction)\n",
    "dataset = dataset.remove_columns(['input_ids', 'attention_mask', 'trunc_instruction',])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3027fa18487417dac68300734fdc730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4739c6495f4be280855e1eff917aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a4c5c339694de1942207339c560230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetname = dataset_name.split('/')[-1]\n",
    "model_name = model_name.split('/')[-1]\n",
    "dataset.push_to_hub(f'nayohan/{datasetname}_ppl_{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_json('/home/uj-user/Yo/HiT5/HCLT/nlpai-lab_kullm-v2_ppl_polyglot.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
